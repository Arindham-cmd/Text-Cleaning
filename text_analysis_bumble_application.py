# -*- coding: utf-8 -*-
"""Text Analysis - Bumble Application.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jgCLQiS78MvEabKepbVAeGj1p_cQD9lv
"""

import pandas as pd
import numpy as np
import re
import warnings
warnings.filterwarnings("ignore")

data = pd.read_csv(r'C:\Users\Arindham Krishna\OneDrive\Desktop\Projects for Portfolio\Data Cleaning\Bumbble App Reviews Data\bumble_google_play_reviews.csv')

pd.set_option('display.max_columns',500)
pd.set_option('display.max_rows',50)
pd.set_option('display.max_colwidth',-1)

data.head()

"""Let us drop the not required columns from the data. """

data = data.drop(['reviewId','userImage','repliedAt','reviewCreatedVersion','at','replyContent','repliedAt'], axis=1)

data.head()

data.info()

data.isnull().sum()

data[data["content"].isnull()]

data["content"].fillna('No written review provided', inplace = True)

data[data["content"] == 'No written review provided']

data.isnull().sum()

data.shape

data['userName'].value_counts()

for index,text in enumerate(data['content']):
  print('Review %d:\n'%(index+1),text)

' '.join(data['content'].tolist())

"""### Word Count of the review content"""

data["wordcount"] = data["content"].apply(lambda x: len(str(x).split(" ")))
data[["content","wordcount"]].head(20)

data.nlargest(10,"wordcount")

"""### Calculating the number of charachters and making a new column with that."""

data['char_count'] = data['content'].str.len() ## this also includes spaces
data[['content','char_count']].head(10)

"""### Will be checking for the stopwords in the content and also for the stopwords, numerics if any available"""

#nltk library for importing stop words
from nltk.corpus import stopwords
stop = stopwords.words('english')

data['stopwords'] = data['content'].apply(lambda x: len([x for x in x.split() if x in stop]))
data[['content','stopwords']].head(10)

"""### If you want to see what are those stop words that are available in the content then for that use the below code."""

print(data['content'].apply(lambda x: ([x for x in x.split() if x in stop])))
#We just have to remove the "len" component in the lambda function and we can see the list here.

"""### Let's make everything look uniform, so will make the content column to lowercase"""

data['content'] = data['content'].apply(lambda x: " ".join(x.lower() for x in x.split()))
data['content'].head()
data.head()

"""### Lets try to remove punctuations, numbers and special charachters """

' '.join(data['content'].tolist())

warnings.filterwarnings('ignore')

def remove_punc_num(data, attribute):
    data.loc[:,attribute] = data[attribute].apply(lambda x : " ".join(re.findall('[\w]+',x)))
    data[attribute] = data[attribute].str.replace('\d+', '')
    return data

data =remove_punc_num(data, 'content')
data_no_punc =data.copy()

' '.join(data['content'].tolist())

"""## you can see the above text plot does not have any special charachters, the puncntuations have been removed, the numerics have also been removed from the textual data

### Now Lets remove the stopwords, we already have list of stop words and count of stop words each review has. lets remove them all.
"""

from nltk.corpus import stopwords
stop = stopwords.words('english')

data.head(20)

data['cleancontent']= data['content'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
data[['content','cleancontent']].head(25)

"""### We can see that the stop words have been removed now.

### Now, lets see if we can do anything to check and improve the spellings of the content.
"""

from textblob import TextBlob #we will use textblob library to check the spellings.

data['contentspell'] = data['content'][:20].apply(lambda x: str(TextBlob(x).correct()))
data[['content','contentspell']].head(25)

"""#### So, if we compare we can see that few contents had spelling mistakes which were rectified but at same time there were inaccurate changes made too. For example -> word 'app' became pp in the contentspell which is not a desired output. Hence, I do not think that it is wise to use spell check function here and let the content remain as it is. 

Also, it can be a time taking task. If you have a huge dataset then you have to wait for good couple of minutes for the code to get executed and that is the reason I have sliced down the number of rows.

#### I will now drop the contentspell column which was created with corrected spellings of content text.
"""

data = data.drop(['contentspell'], axis=1)

data.head()

"""### Now, let's explore Tokkenization, Stemming and Lemmatization. 

Tokkenization will divide the sentence into sequnce of words. 

Stemming will remove the suffices from words, for example it will remove 'ing','ly' and tries to get to the root word.

Lemmatization, Lemmatization is also similar like Stemming but there are many instances where Lemmatization outnumbers stemming in efficiency and that is the reason we prefer Lemmatization. 

We will work on all these three components. 
"""

import nltk
from nltk.tokenize import WhitespaceTokenizer

tk =WhitespaceTokenizer()

def tokenise(data, attribute):
    data['tokenised'] = data.apply(lambda row: tk.tokenize(str(row[attribute])), axis=1)
    return data

data =tokenise(data, 'cleancontent')
data_experiment =data.copy()
data.head(10)

"""### As you can see that we have tokenized the clean content and not the uncleaned one.

### Now, lets explore stemming. There are multiple ways to do the stemming using differnet libraries. I will showcase this using two libraries or I will show the easiest way to do stemming.
"""

#First we will import the library
from nltk.stem import PorterStemmer
st = PorterStemmer()

data["contentstem"] =data['cleancontent'][:20].apply(lambda x: " ".join([st.stem(word) for word in x.split()]))
data[['content','cleancontent','contentstem','tokenised']].head(20)

"""### You can clearly see that there are few misses when comes to efficiency, lets also work on lemmatization and see how it works. """

from textblob import Word

data['contentlemm'] = data['cleancontent'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))
data[['cleancontent','contentstem','contentlemm']].head(20)

"""#### So, as you can see from the 20 rows sample that there are no changes from the cleancontent to lemmatized content but atleast good part is that we do not get wrong ouputs like stemming and maybe in ahead records we might encounter words that are actually lemmatized. Again, keep in mind that Textblob takes couple of minutes to execute the command.

## Here, I will display how you can make the WORD CLOUDS. You can use the lemmatized content or Stemmed Content but make sure that your content or text data does not contain any stop words.
"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def wordcloud(data):
    wordcloud_ent = WordCloud().generate(' '.join(data['contentlemm']))
    plt.imshow(wordcloud_ent, interpolation='bilinear')
    plt.axis("off")
    plt.rcParams["figure.figsize"] = (20,9)
    return plt.show()
wordcloud(data)

"""### Now you can see there are words which are very common and their size of word is large compared to other words like dating app, bumble, match and people and many more but if you think the words which are highlighted does not bring an insight to your analysis then you can anytime drop  """

# stopwords = nltk.corpus.stopwords.words('english')
# newStopWords = ['stopWord1','stopWord2']
# stopwords.extend(newStopWords)

"""### Now, this is how you can extend the stop words but after running this code you will have to run all the other codes especially of lemmatization because we have made word cloud using lemmatized words.  """

from nltk.corpus import stopwords
stop = stopwords.words('english')
morestopwords = ['bumble','dating app','match']
stop.extend(morestopwords)